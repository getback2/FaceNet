{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model, Input\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Lambda\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from mtcnn import MTCNN\n",
    "import cv2\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Initialize MTCNN face detector\n",
    "detector = MTCNN()\n",
    "\n",
    "def load_dataset(data_dir, min_images_per_person=1):\n",
    "    images = []\n",
    "    labels = []\n",
    "    label_dict = {}\n",
    "    current_label = 0\n",
    "    \n",
    "    all_persons = os.listdir(data_dir)\n",
    "    # Randomly select about 60% of the persons\n",
    "    selected_persons = np.random.choice(all_persons, size=int(0.4 * len(all_persons)), replace=False)  \n",
    "    for person_name in selected_persons:\n",
    "        person_dir = os.path.join(data_dir, person_name)\n",
    "        if not os.path.isdir(person_dir):\n",
    "            continue\n",
    "        # List all image files for the person\n",
    "        image_files = [f for f in os.listdir(person_dir) if os.path.isfile(os.path.join(person_dir, f))]\n",
    "        # Check the number of images for the person\n",
    "        if len(image_files) < min_images_per_person:\n",
    "            continue  # Skip if not enough images\n",
    "        label_dict[current_label] = person_name\n",
    "        # Iterate over each image of the person\n",
    "        for image_name in image_files:\n",
    "            image_path = os.path.join(person_dir, image_name)\n",
    "            # Read the image using OpenCV\n",
    "            img = cv2.imread(image_path)\n",
    "            if img is not None:\n",
    "                images.append(img)\n",
    "                labels.append(current_label)\n",
    "        current_label += 1\n",
    "    return images, labels, label_dict\n",
    "\n",
    "def preprocess_faces(images, labels):\n",
    "    processed_images = []\n",
    "    processed_labels = []\n",
    "    for idx, img in enumerate(images):\n",
    "        # Convert image to RGB (OpenCV uses BGR)\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        # Detect faces\n",
    "        results = detector.detect_faces(img_rgb)\n",
    "        if results:\n",
    "            # Find the largest face\n",
    "            max_area = 0\n",
    "            largest_face = None\n",
    "            for result in results:\n",
    "                x, y, width, height = result['box']\n",
    "                area = width * height\n",
    "                if area > max_area:\n",
    "                    max_area = area\n",
    "                    largest_face = result['box']\n",
    "            if largest_face is not None:\n",
    "                x, y, width, height = largest_face\n",
    "                h, w, _ = img_rgb.shape\n",
    "                x1 = max(0, x)\n",
    "                y1 = max(0, y)\n",
    "                x2 = min(w, x1 + width)\n",
    "                y2 = min(h, y1 + height)\n",
    "                face = img_rgb[y1:y2, x1:x2]\n",
    "                # Resize face to 224x224 (ResNet input size)\n",
    "                face = cv2.resize(face, (224, 224))\n",
    "                processed_images.append(face)\n",
    "                processed_labels.append(labels[idx])\n",
    "        else:\n",
    "            continue\n",
    "    return np.array(processed_images), np.array(processed_labels)\n",
    "\n",
    "def create_triplets(images, labels, minperson):\n",
    "    triplets = []\n",
    "    num_classes = len(np.unique(labels))\n",
    "    label_indices = {label: np.where(labels == label)[0] for label in np.unique(labels)}\n",
    "    \n",
    "    for anchor_label in label_indices.keys():\n",
    "        indices = label_indices[anchor_label]\n",
    "        if len(indices) < 2 and minperson > 1:\n",
    "            continue\n",
    "        for idx in indices:\n",
    "            anchor_image = images[idx]\n",
    "            # Positive image (same class)\n",
    "            positive_idx = idx\n",
    "            if minperson > 1:\n",
    "                while positive_idx == idx:\n",
    "                    positive_idx = np.random.choice(indices)\n",
    "            positive_image = images[positive_idx]\n",
    "            \n",
    "            # Negative image (different class)\n",
    "            negative_label = np.random.choice([l for l in label_indices.keys() if l != anchor_label])\n",
    "            negative_indices = label_indices[negative_label]\n",
    "            negative_idx = np.random.choice(negative_indices)\n",
    "            negative_image = images[negative_idx]\n",
    "            \n",
    "            triplets.append([anchor_image, positive_image, negative_image])\n",
    "        \n",
    "    return np.array(triplets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'archive\\\\lfw-deepfunneled\\\\lfw-deepfunneled'\n",
    "include_single_image_folders = True\n",
    "min_images_per_person = 2 if include_single_image_folders else 5\n",
    "\n",
    "images, labels, label_dict = load_dataset(data_dir, min_images_per_person)\n",
    "processed_images, processed_labels = preprocess_faces(images, labels)\n",
    "\n",
    "# Convert dtype and preprocess with ResNet50's preprocess_input\n",
    "processed_images = processed_images.astype('float32')\n",
    "processed_images = preprocess_input(processed_images)\n",
    "\n",
    "triplets = create_triplets(processed_images, processed_labels, min_images_per_person)\n",
    "\n",
    "triplets_train, triplets_test = train_test_split(triplets, test_size=0.2, random_state=42)\n",
    "\n",
    "def split_triplets(triplets):\n",
    "    anchors = triplets[:, 0]\n",
    "    positives = triplets[:, 1]\n",
    "    negatives = triplets[:, 2]\n",
    "    return [anchors, positives, negatives]\n",
    "\n",
    "X_train = split_triplets(triplets_train)\n",
    "X_test = split_triplets(triplets_test)\n",
    "y_dummy = np.zeros((len(triplets_train),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\kod\\FaceNet\\.conda\\Lib\\site-packages\\keras\\src\\models\\functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['anchor_input', 'positive_input', 'negative_input']. Received: the structure of inputs=('*', '*', '*')\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m506s\u001b[0m 6s/step - loss: 0.0937 - val_loss: 0.0717\n",
      "Epoch 2/6\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m466s\u001b[0m 5s/step - loss: 0.0493 - val_loss: 0.0615\n",
      "Epoch 3/6\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m555s\u001b[0m 6s/step - loss: 0.0313 - val_loss: 0.0563\n",
      "Epoch 4/6\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m525s\u001b[0m 6s/step - loss: 0.0196 - val_loss: 0.0523\n",
      "Epoch 5/6\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m529s\u001b[0m 6s/step - loss: 0.0100 - val_loss: 0.0513\n",
      "Epoch 6/6\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m573s\u001b[0m 7s/step - loss: 0.0063 - val_loss: 0.0492\n",
      "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m220s\u001b[0m 2s/step\n",
      "(224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "def create_embedding_model(input_shape):\n",
    "    base_model = ResNet50(include_top=False, weights='imagenet', input_shape=input_shape)\n",
    "    # Freeze base_model layers\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(128)(x)\n",
    "    x = Lambda(lambda v: tf.math.l2_normalize(v, axis=1))(x)\n",
    "    \n",
    "    model = Model(inputs=base_model.input, outputs=x)\n",
    "    return model\n",
    "\n",
    "def create_triplet_model(input_shape):\n",
    "    embedding_model = create_embedding_model(input_shape)\n",
    "    anchor_input = Input(shape=input_shape, name='anchor_input')\n",
    "    positive_input = Input(shape=input_shape, name='positive_input')\n",
    "    negative_input = Input(shape=input_shape, name='negative_input')\n",
    "    \n",
    "    encoded_anchor = embedding_model(anchor_input)\n",
    "    encoded_positive = embedding_model(positive_input)\n",
    "    encoded_negative = embedding_model(negative_input)\n",
    "    \n",
    "    merged_output = layers.concatenate([encoded_anchor, encoded_positive, encoded_negative], axis=1)\n",
    "    \n",
    "    model = Model(inputs=[anchor_input, positive_input, negative_input], outputs=merged_output)\n",
    "    return model\n",
    "\n",
    "def triplet_loss(y_true, y_pred, alpha=0.2):\n",
    "    total_length = y_pred.shape.as_list()[-1]\n",
    "    anchor = y_pred[:, 0:128]\n",
    "    positive = y_pred[:, 128:256]\n",
    "    negative = y_pred[:, 256:384]\n",
    "    pos_dist = tf.reduce_sum(tf.square(anchor - positive), axis=1)\n",
    "    neg_dist = tf.reduce_sum(tf.square(anchor - negative), axis=1)\n",
    "    basic_loss = pos_dist - neg_dist + alpha\n",
    "    loss = tf.reduce_mean(tf.maximum(basic_loss, 0.0))\n",
    "    return loss\n",
    "\n",
    "input_shape = (224, 224, 3)\n",
    "triplet_model = create_triplet_model(input_shape)\n",
    "\n",
    "triplet_model.compile(optimizer=Adam(0.0001), loss=triplet_loss)\n",
    "\n",
    "# Add EarlyStopping callback\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
    "\n",
    "# Train the model for 6 epochs instead of 10, with early stopping\n",
    "triplet_model.fit(\n",
    "    X_train,\n",
    "    y_dummy,\n",
    "    validation_data=(X_test, np.zeros((len(triplets_test),))),\n",
    "    batch_size=32,\n",
    "    epochs=6,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "embedding_model = create_embedding_model(input_shape)\n",
    "embedding_model.set_weights(triplet_model.get_weights()[:len(embedding_model.weights)])\n",
    "embedding_model.save_weights('test40%.weights.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 2s/step\n",
      "1.0480235\n",
      "0.83653784 1.0483295\n",
      "0.9764533\n",
      "0.83653784 1.0483295\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_embeddings(model, images):\n",
    "    return model.predict(images)\n",
    "def create_embedding_model(input_shape):\n",
    "    base_model = ResNet50(include_top=False, weights='imagenet', input_shape=input_shape)\n",
    "    # Freeze base_model layers\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(128)(x)\n",
    "    x = Lambda(lambda v: tf.math.l2_normalize(v, axis=1))(x)\n",
    "    \n",
    "    model = Model(inputs=base_model.input, outputs=x)\n",
    "    return model\n",
    "\n",
    "model_path = 'test.weights.h5'  # Replace with your model file\n",
    "\n",
    "input_shape = (224, 224, 3)\n",
    "embedding_model = create_embedding_model(input_shape)\n",
    "    \n",
    "if not os.path.exists(model_path):\n",
    "    print(f\"Model weights file not found: {model_path}\")\n",
    "\n",
    "embedding_model.load_weights(model_path)\n",
    "\n",
    "embeddings = get_embeddings(embedding_model, processed_images)\n",
    "def compute_distance(emb1, emb2):\n",
    "    return np.linalg.norm(emb1 - emb2)\n",
    "\n",
    "all_same = []\n",
    "labels_set = set(processed_labels)\n",
    "for label in labels_set:\n",
    "    same_person_indices = np.where(processed_labels != label)[0]\n",
    "    if len(same_person_indices) > 1:\n",
    "        idx1, idx2 = same_person_indices[:2]\n",
    "        distance_same = compute_distance(embeddings[idx1], embeddings[idx2])\n",
    "        all_same.append(distance_same)\n",
    "       #print(f\"Distance between same person ({label_dict[label]} ): {distance_same}\")\n",
    "\n",
    "a = 0\n",
    "for i in range(len(all_same)):\n",
    "    a += all_same[i]\n",
    "\n",
    "if len(all_same) > 0:\n",
    "    print(a / len(all_same))\n",
    "    print(min(all_same), max(all_same))\n",
    "else:\n",
    "    print(\"No pairs found for same person comparison.\")\n",
    "\n",
    "all_dif = []\n",
    "labels_set = set(processed_labels)\n",
    "for label in labels_set:\n",
    "    same_person_indices = np.where(processed_labels == label)[0]\n",
    "    if len(same_person_indices) > 1:\n",
    "        idx1, idx2 = same_person_indices[:2]\n",
    "        distance_same = compute_distance(embeddings[idx1], embeddings[idx2])\n",
    "        all_dif.append(distance_same)\n",
    "       #print(f\"Distance between same person ({label_dict[label]} ): {distance_same}\")\n",
    "\n",
    "a = 0\n",
    "for i in range(len(all_dif)):\n",
    "    a += all_dif[i]\n",
    "\n",
    "if len(all_dif) > 0:\n",
    "    print(a / len(all_dif))\n",
    "    print(min(all_dif), max(all_dif))\n",
    "else:\n",
    "    print(\"No pairs found for dif person comparison.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
